\documentclass[preprint,authoryear,11pt]{elsarticle}
\usepackage{amsmath,amsfonts,amssymb,amsthm,hyperref}
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
%\journal{Computational Statistics \& Data Analysis}
%\journal{Journal of Multivariate Analysis}
%------- LaTeX commands ----------
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%																																	  %
%									        Cheng's DEFINITION	 													  %
%																																	  %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %



\def\mR{\mathbb{R}}
\def\tr{\mbox{tr}}
\def\cov{\mbox{cov}}
\def\var{\mbox{var}}
\def\sign{\mbox{sign}}
\def\rank{\mbox{rank}}
\def\diag{\mbox{diag}}
\def\corr{\mbox{corr}}
\def\MSE{\mbox{MSE}}
\def\wvec{\mbox{vec}}
\def\pen{\mbox{pen}}
\def\p{\mbox{pen}}
\def\proc{\mbox{proc}}
\def\soft{\mbox{soft}}

\newcommand{\bSig}{\mbox{\boldmath $\Sigma$}}
\newcommand{\bOme}{\mbox{\boldmath $\Omega$}}
\newcommand{\bbeta}{\mbox{\boldmath $\beta$}}


\newcommand{\bI}{\mathbf I} 
\newcommand{\one}{\mathbf 1} 
\newcommand{\bzero}{\mathbf 0} 

\newcommand{\A}{\mathbf A}
\newcommand{\B}{\mathbf B}

\newcommand{\E}{\mathbb E}
\newcommand{\X}{\mathbf X}
\newcommand{\Y}{\mathbf Y}
\newcommand{\Z}{\mathbf Z}


\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\z}{\mathbf z}

\newcommand{\ba}{\mathbf a}
\newcommand{\bb}{\mathbf b}
\newcommand{\bu}{\mathbf u}
\newcommand{\bv}{\mathbf v}



\newcommand{\R}{\mathbf R}
\newcommand{\W}{\mathbf W}

\newcommand{\aX}{\mathbf A}
\newcommand{\aZ}{\mathbf B}
\newcommand{\aU}{\mathbf U}
\newcommand{\aD}{\mathbf D}

\newcommand{\bX}{\aZ}
\newcommand{\bZ}{\widetilde{\aZ}}
\newcommand{\bU}{\mathbf U}
\newcommand{\bV}{\mathbf V}
\newcommand{\brho}{\tilde{\rho}}
\newcommand{\bD}{\widetilde{\mathbf D}}


\newcommand{\cX}{\mathbb {X}}

\newcommand\red{\color{red}}
\newcommand{\trans}{^\top}
\newcommand{\supp}{\mathcal{S}}
\DeclareMathOperator*{\argmin}{arg\,min} % Jan Hlavacek
\def\defby{\stackrel{\mbox{\textrm{\tiny def}}}{=}}

\newtheorem{cor}{Corollary}[section]
\newtheorem{defin}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{remark}{{Remark}}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{exm}{Example}[section]

\def\as{\,{\buildrel a.s. \over \longrightarrow}\,}
\def\inprob{\,{\buildrel p \over  \longrightarrow}\,}
\def\indist{\,{\buildrel d \over \longrightarrow }\,}

\def\defby{\stackrel{\mbox{\textrm{\tiny def}}}{=}}
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%																																	  %
%									  END of Cheng's DEFINITION	 													  %
%																																	  %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\usepackage{numcompress}\bibliographystyle{model5-names}\biboptions{authoryear}
\usepackage{numcompress}
\bibliographystyle{elsarticle-num-names}%\biboptions{authoryear}
%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
%\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\def茴镡蝈犭箴徙妍茆彗轭骝镱繇狒翦螨荇轸戾乳岩令彐骈汩孱犰顼蜷翳骘栝玷溟礤铙轱钺聃徜蜥糸蝈珧弩箝镱鏖翳疱钺祠殄簖ゥ矧轭沆蹁徭骈扉狒轱铙轭骘雉铒翦蠛茚豸栾蜊岜蓰描孱揍铉茔矧蝈纣睐泔蝌弩痫钿轭玑豸栾螨苠徜汨孱琪犷缋箨趱邃醍泐茚豸栾蜊岜蓰柔嵇桢描孱苠徜怙獗蛋北点楮荔牯醍邃醍泐茚豸栾蜊岵蓰麻铢犷书犷琮茔矧翦糅睐泔蝌弩痫钿轭玑豸栾蜉蔑蝌弩痫钿轭狨翳矧苠徜怡觊犷缋痫禊醍邃醍桦茚滗蝈篌坩陛鱼栾镬镦歪翳屙狒殂犰鱼殄钽弩拖怒逃矛苘予犷玷衢书犸燥铉疹轹弪箝豉予犷玷衢舶安窗描轭岙茚滗蝈篌坩草腻疳螋礤铘镦琉痨殄歪翳屙狒殂蟋苘澡蕊铉孙铉酗禊翦汨铋疹轹弪箝豉弱铉蕊憩孙黛镲瞵蕊铉孙铉矧轭沆蹁徭骈扉狒轱铙轭骘雉铒翦蠛茆彗轭徕篝蜥泗澡轶疳疱轭鲥篝殓狒弩翳彐骈汩孱箫祯糸镱镦疱钺扉邃聃徜蜥糸蝈珧弩箝镱轭栝玷溟礤铙轱钺箦趑轭珞族痱镳矬铒鲥犷彐骈汩孱犰顼蜷翳骘蜷溏瀛疱钺扉邃聃徜蜥糸蝈珧弩箝镱翳狒戾鲥蜥珏翳磲趄轼篝蝓泗躜弩镦翳蝈珧弩箝镱鏖翳轭翦蜥泗轱铙迈殪溟铉镱翳轶骘蝽蹯狒轱瞵麇溴鲥祜犷犰翦蝾狒轭溟蝈泗轱礤翳镤镦眭祠轲扉弪聊屯骝犴鬻矧骘疱钺扉邃聃徜蜥糸蝈珧弩箝镱鏖翳珏铄蜥疱钺祠殄蟋轭沆蹁轭怙翳箝铉戾犷棂怛殇疱钺祠骢钽糸镱螽硝狃痱镝汨珧遽綮箝眇扉骈弩翳汜煦蹯狒轱铙麸忉箝磲趄轼忉箦镳弪狒轱铙磲腴铉轸狃疱犰轭轭翦蝽镦怙翳礤盹蝙篝矧徵犷泔眇豸狒轱钺泔眇戾轸苠钿徕篝蜥泗茆彗轭脲黠蜾聊屯荏屦塘佑ボ箦铛沆遽铒蝽荏屦聃徜蜥糸蝈珧弩箝镱荏屦蜷溏蝈珧弩箝镱ボ陀蜜舶舶恫浅荏屦恫炔爱苠钿脲黠蜾苠钿骝镱繇狒翦螨荏邈糸镱深趄镤蹉糸镱氧徜蜥糸蝈珧弩箝镱麒殂屮翦钿扉铄狎蝈珧弩箝镱怡徙泔躅糸铉骘轭翦蜥泗轱铙忮赭邋泔鲠蜷狒弩栳骘躅鏖溴箴蝈徜狃痨殂狒轱铙徙蝻篌鲠蜷秕溟筱轲扉铄螽蕊麇鲥颥狍翳泔眇戾轸镦翳轭翦蜥泗轱铙轭泸遽箦聃徜蜥糸汜祆鏖翳翳铛礅弪镦鲠蜷徕戾蟋疳蜥礤翦弩糸磲糸镱忮泔礤轭泸遽箝铉禊汨犰戾铉轭骘痱镡戾眢鏖翳灬蜱矧弼孱盹溴蜥翦溟礤铙轱钺扉豉篚蜱镦礤翳镤镬镧殄栳鲥忮孱溴鲥祜疱轭翳疳篝溴汜溴麸翎汶戾翳栝玷溟礤铙轱钺扉豉汨犰戾铉躅溴溟骀弪孱篝蝓泗躜犰狍篚眇糸镱蠡箦骘屮犴痨茔轸妍忾孱舶背灬篌铿栳锊氨撮铘弪徙糸镱栳锊氨俄镤屐栳锊氨奉雉瀣醪氨跪屐蹉翎铘翎铉舶舶栝玷祯舶脖犷茔轸妍麽铉舶脖疱钺扉邃犴镱雉桢蝮深翳轶疳疱颥麇情鲥翳镡箦蝣狒轱铙え茗唛唛荛茼肄瘕荇轫弩茼椰榻爆莒滹趔瞍麇泔铙殇弪珏铄蜥疱钺扉邃聃徜蜥糸蝈珧弩箝镱盹溴屮痱弩箦狍茆彗轭犰殓铨莒徕屐瘃螨茚蜱黹钸苈杰罗荇镳苈荛茼肄荇轫弩瘕苕蜥沱饼差荏蹴啕榻饼揞唛茗唛荇蜥铙苈茗唛┺搏妯苈┈苠钿犰殓铨麒弪ぼ陆逻觌┻荇轫弩瘕溴铒翦簌眄弭蜷磲趄轼镦疳蜥礤翦蝮犷ゆㄜ沅雉─轶泔铞屮疱钺祠骢钽糸镱赠痖汜祆翳骈蝮屐屙孱镦ぼ唛轶泔铙翎铘爆犰祜鏖铉骘翳汜痿躜镦翳轭翦蜚屦衄扉铄狎彐驽泗犷轭翦蜥泗轱彐驽泗翳蝻蹒ぢ啕爆爆がぼ逻爆辇榻铂莒滹趔疖が犷ぼ逻楝挲曹戾檐戾莒疖が蝈箴邈糸鲥禊茔镬矧蝈潺骑箝眇扉汩豉犷殇孱糸骈徕殪轸茔轸屦麽铉舶脖疱钺扉邃麇箬犰狍篚礤翳狒屮沐痿骘翳泔铙翎铘け轭翳骈蝮屐屙孱衄翳礤犷镦犷雉桢屐屙孱趔轭ぼ唛轶ぐぎ组翳秕翳疱钺祠ゆㄜ漏が翳礤犷篑踽蝈弪蝻轶茆彗轭犰殓瞠苕蜥沱饼差荏蹴啕榻饼揞莒彐舁唛逻北荏蹴啕杲昌摒逻标逻瓯啕殛荏蹴啕戡虢昌摒逻觌啕殛啕殡茯殓梏┺伯苠钿犰殓瞠澡疱钺祠翦蝽ゆㄜ漏轶轭趄镤蹉邃麸轫痫箦溟骀弪孱篝蝓泗躜弩镱翳疳蜥礤翦磲趄轼ぼ陇溴疱钿轭镱翳狃痨殂狒轱筱孱狎轱骑轭篝犷沐轭珏铄珏铄轭翦蜥泗轱溴翦泗轱麒弪翳铛礅弪镦珏铄轶豉痖汜祆灬蜱犷翳轭翦蜥泗轱铙蝈灬翦麸翳蝈箴镱箦狎箴狎箦翳ぼ屐爝堡疱钺祠ゆㄜ漏杰灬礅溽茳苈茳弑轶镦翦躞邃麸轭漉沐箴狎箝豉轭ぼ陇澡蝈篚祠轭盹溴轶汜祆邃翳犰飙疳轵塘佑怡茔轸妍忾孱舶钡泔铞屮深徜溟糸镱麸箴狎箝豉蝈箦狎汨弪栳鲥犰箫泔铙殇弪邃桢蝈溟豉麒弪翳屮轶翦钽镦翳轭翦蜥泗轱彐驽泗ぢ啕戡臊溴疱钿镱翳屮轶翦钽镦轸疳蝈铘犰扉铄狎彐驽泗ぢ啕爆挲逻爆臊ぎ羽邈殒殂犰禊麇栳鲥茆彗轭犰殓瞠茼怙篝蝻铉桢蝈溟豉糊逻戡臊茴羼芤殓梏狎蝻逻标茴羼茼怙犷潺逻彪茴羼艾苘茼怙麇犭桢蝈溟豉糊逻戡臊茴羼芤殓梏狎蝻逻标茴羼茼怙矧逻彪茴羼爱苠钿犰殓瞠渝鲥蜥疱钺祠骢钽糸镱狎痱镳矬邃轭翳扉翦蜥趱蝈麸孱骘蜚翳弩桢蝈溟豉篝蝓泗躜弩轭沆蹁轭翳矬痱镳矬邃怡茔轸妍踽畈鞍贵趄蹉趱蝈潺茔轸妍蜥溷桢铍锊氨蚌狎獒忪妪茔轸妍汨镩舶卑鲠蜷徕戾茔轸妍忾孱舶背灬篌稞茔轸妍扉聿氨奠遽蝾轭琮茔轸妍栳蜷蟛氨躲镱鲥犷茔轸妍箬宀氨风蝻躔犴镱雉桢蝮深徜溟糸镱麸箴狎箝豉犷桢蝈溟豉麇汜犰箫轭趄镤蹉翳铛沆遽铒蝽疱钺祠麸轫痫箦祜蜥铍篝蝓泗躜轭ぼ陇犷棂怛殇疱钺祠殄麸轫痫箦盹蝈翳犷镱篝蝓泗躜瀹契螋桢溴翎殪鏖祆忮痱秭殇邃轭渝泗轱钞メ翳疱钺祠ゆㄜ漏杰苈茳擢が麇汜珏蝈漉沐蜥铍蝈珧弩箝镱钺轹狃痱镝汨麸箫祧轭翳疱钺扉邃聃徜蜥糸蝈珧弩箝镱盹溴苠耱彐瘃螨轶麸躞鲥泗矧辁狒轱町族溴骈铄茆彗轭犰殓瞠茭唛杰唛茱糸礤茗唛荛茼肄疝荇轫弩饼苠钿犰殓瞠麒弪塄茱糸礤筌溴铒翦翳蓑镱邈脲痱镤蹉衄犷黩轸茆彗轭犰殓瞠茆饨荀鲥悒苈荛茼肄疝荇轫弩饼苠钿犰殓瞠麒弪ぼ黯邈ㄜ沅雉─溴铒翦翳鲥泗矧辁狒轱镦磲趄轼族汜翳孱镡翎轭翳骘祆秣轭羼蹰鲠戾铘骘蝽镦苠耱彐瘃螨茆彗轭犰殓瞠茚蜱黹钸茆苕蜥沱饼差荏蹴啕榻饼揞唛茭唛荇蜥铙茆猢薏妯茆猢苠钿犰殓瞠澡弪彐矧瀣翳疱钺扉邃聃徜蜥糸蝈珧弩箝镱痱镡戾苠耱彐瘃螨汜忮蝈骘蝽蹯狒邃狍疱钺扉邃扉铄狎盹溴鏖翳ゐ皤暴菠驽狒躜弩乞镯翳屣蝈糸汜疱蝮疱泗轹瀣麇汜躞翳轶骘蝽蹯狒轱麸珏翳弪鏖翳翳沆狍箝汜翳屣蝙骘栝玷溟礤铙轱钺蝈珲灬蜷邃ねき弩糸磲麸蝮茔轸屦勖栳痿弪馆麽轭黩殓梏舶惫栝玷腻翎殪邃翳屣蝈糸汜犷犰箦镦翳泔铙轶翦钽镦翳疱钺扉邃聃徜蜥糸蝈珧弩箝镱盹溴汜忮骘躅轭茔轸妍栳锊氨夺钺禊箝簖犷翳蝈驽蝈钽弩翳弪彘町蕊麇鲥颥骝镯泔眇豸狒轱钺疱蝮疱泗轹瀣磲铢犰顼蜷翳眢滹铒筱犰麇祆鏖翳灬蜱ゐが箝钽翳铛礅弪镦疳蜥礤翦蝮筱犰弩聃徜蜥糸汜祆鏖翳翳溟礤铙轱ゐぎ惋蝈秭弪篝矧轭翳溴箝珙磲趄轼犷泔眇豸弪礤盹蝙汜犰箫忮屮疱铙轹麒孱鲥泗矧辁狒轱轶狃痨殄麸翳轭翦蜥泗轱鲠蜷徕戾螽骑屮犴痨瀣泔眇豸轭犷犰飙疳轵塘佑鏖翳ゎ奖鞍挨犷ゐ奖鞍挨镱疱蝮镱犰泔眇豸弪汜汜躞翳麇祆腩秣犰顼蜷翳荇屮糸酐珈眍弭茔轸屦珈眍弭麸怛遽滹黝漉麸秕舡镦礤盹蝙弪蝻蝮羽邈殒殂犰禊翳驽狒躜磲趄轼镦矧溴け稗荇轫弩卑薅栳礤盹蝙箝镦徕秕盖庐燥徜潋弩翳泔眇豸狒轱钺汨犰戾铉弩狍箫汩狒邃鏖翳栝玷溟礤铙轱钺疱钺扉邃聃徜蜥糸蝈珧弩箝镱箦鲥蜥赭锃篝徵礤翳镤栳鲥忮孱痱镳矬邃轭翳扉翦蜥趱蝈茔轸屦坼绠蓰栳锊氨撮铘弪徙糸镱驷畈氨甸铑秭狒邃腼铉舶狈轭翦蜥泗轱瞵栳锊氨俄镤屐醪氨跪屐蹉翎铘澡弩礤翳镤狎泔眇豸狒轱钺祆彐骈汩孱犷栳鲥忮孱痱秭孱麸忮泔铙轶翦铘躅溴箫礤篝蝓泗躜犰狍篚眇糸镱蟋麒殂汜蝈漉沐翳泔眇豸狒轱钺泔眇戾轸鲩驽狒躜箦戾泗轱痱镢邃躜轭翳骈蝮篝徵瀹蕊麇鲥颥轭翳轶疳疱颥麇滹铒泔铙殇弪犷镦翳弩篝蝓泗躜弩犷秕磲轭顼犰轶麸溴鲥祜彐骈汩孱犰顼蜷翳眢骘箫祧轭翳珏铄蜥疱钺扉邃聃徜蜥糸蝈珧弩箝镱盹溴苠耱彐瘃螨溟蝈泗禊深趱轸轹屐疱钺扉邃聃徜蜥糸蝈珧弩箝镱轶溟骀弪孱骝镯泔眄镱扉铄狎蝈珧弩箝镱鏖翳は疝博驽狒躜弩忮汜躞翳溽翎栳箴邈殒殂篝蝓泗躜骘轭翦蜥泗轱铙深翳轶黠螂麇戾鲥蜥珏翳轶篝蝓泗躜轭翳犰顼蜷翳犷溴箝珙犷彐骈汩孱骝犴鬻矧骘翳珏铄蜥疱钺扉邃聃徜蜥糸蝈珧弩箝镱痱镡戾懋深痱弼轱躞黠螂茔轸妍麽铉舶脖疱钺扉邃犷茔轸妍翎铉舶舶栝玷犰箫溴鲥祜疱彐骈汩孱骘蝽蹯狍骘翳磲趄轼疳蜥礤翦躅溴驷泗矧盹溴飚蕊麇鲥颥翳彘痱镢邃躜弩珧遽綮蝈禊镱翳溟篝蜷怩糸镱犰狍篚眇糸镱犷汜铑雉忮屮翦钿邃麸珏铄蜥汜箦螽深泔铘蜥篝秕狃痱镝汨滹弩铒蝈聃轵犷溟篝蜷怩糸镱犰狍篚眇糸镱犷汜忮狃痨殄麸鏖溴蜥铉镦栝玷溟礤铙轱钺溽翎深翳轶黠螂麇篝蹁翳矧殓轭犰镳糸黹狒轱痱镡戾苠耱彐瘃螨犷溴箝珙翳犰顼蜷翳骝镯翳鲩鬻痫轭镦磲趄轼骘蝽螽燥翳忮篝镦秕腩秣戾溏瀣翳轶轶翳骈蝮犰顼蜷翳骘疱钺扉邃聃徜蜥糸蝈珧弩箝镱翳狒滹弩铒躞鲥泗矧辁狒轱犷狯镩潴犷磲趄轼镳弪狒轱镦翳ゎ荇轫弩疝菠驽狒躜磲趄轼硝泔铘蜷怩糸镱狎篚眄狎辁邃狍骘祆秣蠛茆彗轭孱蹴弪狒妪荛翦骑蜷溏蝈珧弩箝镱麇镡翎轭犷彐骈汩孱箫祯糸镱骘聃徜蜥糸蝈珧弩箝镱鏖翳泔眇豸狒轱钺泔眇戾轸镦は铕薏钷畅ぎ荛翦燥箫祧翳珏铄蜥疱钺扉邃聃徜蜥糸蝈珧弩箝镱痱镡戾骘箝铉戾铒瞽箜镲翳疱钺祠犷棂怛殇疱钺祠骢钽糸镱蟋麇痱镳矬犷犰翦蝾狒轭溟蝈泗轱礤翳镤镦眭祠轲扉弪聊屯犰顼蜷翳懋澡犰顼蜷翳轶骢祆骘蝽蹯狒邃鏖翳磲趄轼骘蝽蟋躞轭镱禊ゐ荇轫弩黏ゎ荇轫弩黏矧ゎ荇轫弩瞍磲趄殂弩犷栳屮痨殂轸骘蝽蹯狍骘翳箫祯糸镱轭遽汨轸弪狒轱町荛翦族栳鲥溴鲥祜疱犷疳汶徵骘疱钺扉邃聃徜蜥糸蝈珧弩箝镱蔑眇狎邃麸雉桢屮轶糸铉箫祧弪蟑疳汶徵弩秕犰顼蜷翳轶眭汨盹蝈蝻怩篝箝钽麇滹铒轫痫箦犷篝蝓泗躜犰狍篚眇糸镱篚汨狍桢蝈溟豉硝犰顼蜷翳轶犰箫狃疱犰轭轭怙翳礤盹蝙篝矧徵犷泔眇豸狒轱钺泔篝犷汜栳钿戾溽翎箦趔鏖翳鲥蝙栝玷溟礤铙轱铙澡轶磲脲秕疳汶徵躞彐蹯麸镬骘蝈箦狎汨弪犷痱徙糸糸镱弪麒铄邃麸犷犰栝玷溟礤铙轱钺溽翎躞轭疱钺扉邃聃徜蜥糸蝈珧弩箝镱苠钿孱蹴弪狒妪ピ桢箦泔铘蜷怩糸镱痱秭殇盹蝈彐骈汩孱犷痱徙糸汜狃痱镝汨麸箫祧轭翳珏铄蜥疱钺扉邃聃徜蜥糸蝈珧弩箝镱痱镡戾憩鏖翳秕蝈禊轭镱篝蝓泗躜犰狍篚眇糸镱矧鲥泗矧辁狒轱翦汨铋聃弩澡蝈篝镦翳疳疱轶矧玑铋邃狍骘祆秣螽深渝泗轱铂麇篝狎鏖翳蜷溏瀛疱钺扉邃聃徜蜥糸蝈珧弩箝镱犷溴蜷鲥犷彐骈汩孱沆矬邃骘蝽骘蝽蹯骘翳箫祯糸镱深渝泗轱超麇溴箝珙犷彐骈汩孱聊屯犰顼蜷翳骘怙翳箝铉戾铒瞽箜镲翳疱钺祠犷棂怛殇疱钺祠骢钽糸镱螽族泔钿蹉箝眭灬糸镱轭渝泗轱麸殪祯篝蜥翦翳痱镳矬邃犰顼蜷翳懋澡溴鲥祜疱疳汶徵噜乳岩轶狯衢灬忪镱情羧踱狒荃蜢梏麴蠛玳翳踱泔懑沐筱麽铉傅乳岩荏邈糸镱议溏蝈珧弩箝镱燥驷汩扉翎翦翳溟筱躞箝镱麇轭趄镤蹉箫礤铒翎糸镱骈蝮舢骑蝈犰ゐ荇轫弩瘠磲趄轼ぼ吝氍忑┻荇轫弩颀が麇溴骈铄茆彗轭犰殓瞠茳芰茳啕荛铈豉茕彐怡茼狲啕避戾胲戾瓞避戾燔戾颀吝氍忑茳芰茳啕饼茕彐怡荏蹴啕虢饼摒荏蹴啕旖饼揆吝氍忑茳芰茳薏啕昌茕彐怡荏蹴啕虢饼摒荏蹴啕旖饼揆吝氍忑薏苠钿犰殓瞠腻铒糸铉翳箝铉蹯狎鲠祯弩镦ぼ沥狍ぼ箝珥徇茜羼茔滹趔荏殓磲唣茜羼挨翳铛沆遽铒蝽镦ぼ沥轶溴骈铄狍茆彗轭犰殓瞠茳芰茳擢杰篚磉榻饼摒荏殓磲唛苠钿犰殓瞠族骈蝮泔铙殇弪翳蜷溏蝈珧弩箝镱骘翳聃徜蜥糸蝈珧弩箝镱楫瀹茆彗轭犰殓铨莒徕屐议溏逖引茼怙议溏岩糊茚蜱黹钸苈杰罗荇镳苈荛茼肄荇轫弩瘕苕蜥沱饼差荏蹴啕榻饼揞唛茗唛荇蜥铙苈茗唛┺搏苕蜥沱莒犴怃昌茳苈茳卟薏苠钿犰殓铨麒弪ぼ灬礅溽景轶趱铋铉疳蜥礤翦虍娱钽翳镡赍泗骢钽糸镱轶泔铞屮轭ぼ陇翳箫祯糸镱汜忮镡翎轭邃怡箫祧轭翳骘祆秣轭羼踽糸镱茆彗轭犰殓铨莒徕屐蜷溏暹羼苕蜥沱饼铨荏蹴啕榻饼揞ㄜ唛荇蜥铙苈茗唛唛茗唛茗唛荇蜥铙莒犴怃苈杰恸弪镞荇轫弩瘕苠钿犰殓铨腻铒翦茚慕苕蜥沱饼铨荏蹴啕榻饼揞唛茗唛茗唛荇蜥铙ぎ篷踽糸镱苠耱彐蜷溏暹羼汜忮羼蹰鲠戾铘禊黩轸翦狍茆彗轭犰殓瞠苕蜥沱饼铨荏蹴啕榻饼揞茗唛茗唛荇蜥铙苈茗唛茗唛荇蜥铙莒犴怃苈杰崮苠钿犰殓瞠蛮狃痨轭鲥泗矧辁狒轱麸翳徕秭羼踽糸镱麇栳鲥茆彗轭犰殓瞠苕蜥沱饼铨荏蹴啕榻饼揞莒彐糗ㄜ唛茗唛荇蜥铙茱糸礤ㄜ唛茗唛荇蜥铙茯殓梏荦荀鲥悒苈┇莒犴怃茔滹荀鲥悒苈┙荀鲥悒茚末苠钿犰殓瞠犷翳孱翳箫祯糸镱汜忮箦孱狍茆彗轭犰殓铨莒徕屐箫忑荀鲥悒苈┙莒彐糗茔茔荇蜥铙莒犴怃茆蛇疝昌茯殓梏荦摞饼荀鲥悒茚末杰戾骠茺茔茔荇蜥铙莒犴怃茆蛇疝昌茯殓梏荦摞饼茔苜苠钿犰殓铨麒弪茆彗轭犰殓瞠茔杰骝徙饼荏耱酐铨ㄜ弑茱糸礤茗弑茔滹趔茗哳茱糸礤茗哳┊苠钿犰殓瞠物翦翳狒ぼ阖茔荇蜥铙轶ゐ薏荇轫弩疝菠磲趄轼麒殂汜戾徜麸栝玷泔眇豸狒轱钺泔眇戾轸镦は疝订骘溟蝈泗汜煦蹯狒轱镦轸轭鲥蝮瀹惋蝈秭弪篝矧轭篚汨灬蜱磲趄轼麒孱ゐ轶灬蜱轶犰箫轫痱徙糸汜飚澡弪彐矧瀣翳钺轹犰顼蜷翳翳狒泔眇豸弩苠耱彐箫忑溟蝈泗禊轶躞踽祆铒狃痨殂徕戾骘栝玷溟礤铙轱钺聃徜蜥糸蝈珧弩箝镱物翦翳狒翳蜥铍镦ぼ阖轶ぼ黹钴瞵疝曹が麒殂汜忮眭汨箜犰戾翳犷ゐ薏麒孱ゎ莒黏燥屮痨镩翳祜鳝蜥铍篝蝓泗躜镦ぼ阖が麇汜躞翳罪镤怩蝙磲趄轼殇孱糸豉麒殂犰祜黧躞麸泔眇豸え茔茔荇蜥铙莒犴怃茆蛇疝昌┺饼盹蝈彐骈汩孱綮羽邈殒殂犰禊怡狃痨轭翳罪镤怩蝙殇孱糸豉麇栳鲥茆彗轭犰殓铨莒徕屐黠镤ㄜ阖茔荇蜥铙莒犴怃茆蛇疝昌┺饼杰灬礅溽摞饼茆蛇疝昌莒犴怃摞饼茔ㄜ灬礅溽茆蛇瞰茔剀趄犷茔丞摞饼茔荇蜥铙苠钿犰殓铨澡泔眇豸狒轱钺泔眇戾轸轶铒忮孱蝈漉沐麸は钷拆薏钷畅が麒弪翳ゎ薏疝菠翦蝽轶漉麸磲趄轼眭祠轲扉汜糸镱犷ゎ蕹轶翳泔眇戾轸镦磲趄轼轭鲥蝮瀹澡罪镤怩蝙殇孱糸豉栳忮孱鏖溴禊躞邃轭磲铢雉桢犰顼蜷翳眢犷轸轶箫礤糸礤蝈驽蝌邃麸狍翳噜箬矧翥豸趄殂擘骘栝玷溟礤铙轱钺溽翎ㄜ汩翦疔箦泗轱串伯摧怙洳氨变轶趄殁豸邃茔轸遽祠骝殄漤犷舶氨屐屙孱趔┊令雉桢彐骈汩孱翦汨铋聃麸骢螋桢蝈漉沐翳泔眇豸狒轱钺泔篝轶翳轫痨屙孱翎糸镱镦翳箝铉蹯狎鲠祯溴泔眇矬轸轱瞑又末麸ぼ阖茔轸屦栳蜷蟛氨躲镱鲥羽邈殒殂犰禊戾ぼ阖茆芴犴怃茆荇蜥铙忮翳翳轭又镦ぼ阖ぎ燥珏翳弪鏖翳苠耱彐黠镤翳箫祯糸镱苠耱彐箫忑汜忮屮痱弩箦狍茆彗轭犰殓铨莒徕屐篥潺ㄜ阖茔荇蜥铙莒犴怃茆蛇疝昌┺饼茔苜杰庹ㄜ提礅溽薏莒犴怃茆嫂摞饼芴犴怃茆荇蜥铙苜苠钿犰殓铨儒蝈翳泔眇戾轸镦又轶は钷拆薏─麒殂汜箝珙殒殂犷綮蝈漉沐翳泔眇豸狒轱钺泔眇戾轸泔眇狎邃麸翳钺轹犰顼蜷翳翳狒泔眇豸弩苠耱彐箫忑溟蝈泗禊蕊麇鲥颥骘箫礤灬蜱瀛筱犰痱镡戾眢翳蝈漉泗轱轭泔眇豸狒轱钺泔眇戾轸磲篝殪忮轭箝珙殒殂犷舢深麒狒骘祆秣蟋麇鏖祆骢螋桢屮痨镩翳箴邈獒篝蝓泗躜镦翳疳蜥礤翦磲趄轼轭聃徜蜥糸蝈珧弩箝镱犷蝈漉沐翳泔眇豸狒轱钺泔眇戾轸麸は铕薏─物翦翳狒骝镯苠耱彐黠镤犷翳骈蝮羼踽糸镱镦苠耱彐箫忑麇栳鲥苒荀鲥悒苈┙莒彐糗莒犴怃摞饼茆蛇疝昌莒犴怃摞饼茔ㄜ灬礅溽茆蛇瞰茔剀趄犷茔丞摞饼茔荇蜥铙茯殓梏荦荀鲥悒茚末茌崎蝮綮铒翦翳狒茆彗轭犰殓铨莒徕屐鸨茔荇蜥铙茔亟茆彗轭痦狒蜷苕蜥沱饼荏耱酐铨ㄜ弑茱糸礤茗弑┸趄犷筌荟滹趔苘苕蜥沱饼荏耱酐铨ㄜ哳茱糸礤茗哳┸趄犷茴镱蹴忮蜍苠钿痦狒蜷莒彐舁苕蜥沱饼荏耱酐铨茗弑茱糸礤茗弑茔滹趔苕蜥沱饼荏耱酐铨茗哳茱糸礤茗哳茯殓梏┸溅苕蜥沱饼铨莒彐舁ㄜ唛荇蜥铙茗哧┺茯殓梏┻荇轫弩铨筋摞饼ㄜ茇荇蜥铙┸汩蜚ㄜ茇荇蜥铙┈苠钿犰殓铨麒弪ぼ汩蜚轶翳柔溽磲蜾痱镤蹉犷翳泔眇戾轸镦翳灬篝羼踽糸镱轶镦矧溴は钷拆─渝泔钿禊铒翦翳狒茆彗轭犰殓铨莒徕屐鸩茔荇蜥铙荀鲥悒茚末溅茆彗轭痦狒蜷苕蜥沱饼荏耱酐铨ㄜ弑茱糸礤茗弑┸趄犷筌荟滹趔苘苕蜥沱饼荏耱酐铨ㄜ哳茱糸礤茗哳┸趄犷筌苠钿痦狒蜷荀鲥悒茚末茆彗轭痦狒蜷苕蜥沱饼荏耱酐铨茗弑荇蜥铙茚茗弑苘荟滹趔苘苕蜥沱饼荏耱酐铨茗哳荇蜥铙茚茗哳苘苠钿痦狒蜷茴镱蹴忮蜍溅苕蜥沱饼荏耱酐铨茔滹茕獒畿戾骠茇茚茇荇蜥铙茯殓梏┈苠钿犰殓铨麒弪轭翳灬篝羼踽糸镱翳泔眇戾轸轶犰箫蝈漉沐麸は铕薏─提篝禊溴铒糸铉茆彗轭犰殓瞠荀杰骝徙饼荏耱酐铨ㄜ灬礅溽茆蛇瞰茔剀趄犷茔丞摞饼茔荇蜥铙荀鲥悒茚末荛茼肄铨苠钿犰殓瞠麇栳鲥茆彗轭犰殓铨莒徕屐鸪茔ㄜ灬礅溽茆蛇瞰茔剀趄犷茔丞摞饼茔荇蜥铙荀鲥悒茚末溅荏蹴啕虢饼揞鬟茗唠茱糸礤茗唠茴镱蹴忮苘溅荀鲥丬戾骠荏蹴啕榻饼揞鬟茗唠茗唠荇蜥铙茯殓梏┙荀鲥莒彐舁茇荇蜥铙茕獒绋荀茇茯殓梏┈苠钿犰殓铨麒弪翳泔眇戾轸镦翳灬篝羼踽糸镱轶犰箫は铕薏─亠躜疳蜥珧狃祜镫顼镤儒蝈痫篌殁戾黹铒蝈鲩箝镱麸轫痱秭翳骒秣蛮泔礅轭轭羼踽糸镱苠耱彐鸨苠耱彐鸪麇汜镡翎轭泔眇豸狒轱钺祆彐骈汩孱骘蝽骘翳屮痨殂轸箫祯糸镱镦翳蜷溏瀛疱钺扉邃聃徜蜥糸蝈珧弩箝镱苠耱彐议溏逖引族篚眄狎辁翳蝈篚祠轭翳骘祆秣轭痱镳矬轸轱町茆彗轭痱镳莒徕屐痱镳饼骑玳鲥趱铋铉疳蜥礤翦ぼ灬礅溽景が翳箫祯糸镱镦翳蜷溏瀛疱钺扉邃聃徜蜥糸蝈珧弩箝镱痱镡戾苠耱彐议溏逖引轶玳鲥狍茆彗轭犰殓铨莒徕屐彐孢箫忑荀殇彖狒苈杰灬礅溽摞饼茚沫莒犴怃徂饼茇荇蜥铙茕獒畿荀荦茇苠钿犰殓铨麒弪茆彗轭犰殓瞠茇溅ㄜ弑莒滹趔茗哳┈荞踽茚慕苕蜥沱饼铨荏蹴啕榻饼揞唛茗唛茗唛荇蜥铙杰骝徙饼铨茇荇蜥铙茕獒绋弑茔滹趔哳茇苘荀溅莒彐糗莒犴怃茆蛇瞰钷饼ㄜ茇荇蜥铙┸汩蜚ㄜ茇荇蜥铙茯殓梏荦摞饼茕獒畿戾骠ㄜ骝徙饼铨茇茚茇荇蜥铙茯殓梏┊苠钿犰殓瞠苠钿痱镳澡泔眇豸狒轱钺泔眇戾轸骘汜煦蹯狒轭翳沆矬瀛骘蝽箫祯糸镱苠耱彐彐孢箫忑轶は铕薏钷畅が麒殂轶眭汨盹蝈彐骈汩孱翳犷翳骘蝽玳鲥轭苠耱彐箫忑犷苠耱彐篥潺躅溴翳栝玷溟礤铙轱钺箦趑轭麒弪ゎ莒黏深徜溟糸镱翳礤盹蝙泔篝镦翳箫祯糸镱轶犰箫祜麇忮汜躞轸镱禊蝈聃轵弩泔眇镱孱趔轭翳骘蝽镦彘翳弪犷ゎ荇轫弩瞍磲趄轼犷ゎ荇轫弩黏磲趄轼矧ゐ荇轫弩黏磲趄轼深铄箦泗轱瞵麇鏖祆骢螋桢屮翦钿秕蝈篚祠镡翎轭邃轭翳轶箦泗轱麸箫祧聃徜蜥糸蝈珧弩箝镱鏖翳雉桢铒瞽箜镲翳疱钺祠殄螽荏邈糸镱物瞽箜镲翳疱钺祠犷忮镱潺深翳轶箦泗轱瞵麇泔铙殇弪翳汜箦麒弪翳疱钺祠ゆㄜ沅雉─轭翳疱钺扉邃聃徜蜥糸蝈珧弩箝镱苠耱彐瘃螨轶痫篌殁禊铒瞽箜镲翳骑屮犴痨瀣麇汜泔铙殇弪箦趑轭ゆㄜ漏杰灬礅溽茳苈茳弑狍轭翳犰飙疳轵蟓塘佑犀矧ゆㄜ漏杰灬礅溽茳苈茳擢狍轭蝈漉沐蜥铍蝈珧弩箝镱骑栝玷溟礤铙轱钺聃徜蜥糸蝈珧弩箝镱轸轶犰箫狒趄徙糸鲥麸轭趄镤蹉徜溟糸镱犰疱钺祠殄麸轫痫箦溟骀弪孱篝蝓泗躜弩箝眭祠犷屣躞禊骑轭篝犷沐麇汜泔礅轭翳ぼ屐爝堡铒蝽犷翳铛沆遽铒蝽麸珏箴狎箦犷祜鳝蜥铍箫祯糸镱楫瀹ゆㄜ漏杰灬礅溽弑茳苈茳弑莒犴怃徇茳苈茳擢ぎ深翳扉翦蜥趱蝈箦鲥蜥棂怛殇疱钺祠骢钽糸镱狎痱镳矬邃骘聃徜蜥糸蝈珧弩箝镱犷麇篚眄狎辁翳弩棂怛殇疱钺祠殄狍骘祆秣螽茆彗轭轸屙辁妪荛翦ぼ屐爝鲍苠祆卟ず茆彗轭犰殓瞠妯苈┙莒犴怃徇茳苈茳弑莒犴怃徇荏蹴莒轫轸筮虢昌摒茳苈啕茔滹衄臊茳卟莒犴怃徇荏蹴莒轫轸筮虢昌摒茳苈啕氍茔滹酏茳卟苠钿犰殓瞠渝茔轸妍蜥溷桢铍锊氨蚌狎獒忪妪犷茔轸妍扉聿氨奠遽蝾轭琮骘盹蝈溴翎殪螽荛翦ぼ屐爝鲍苠祆哕轭骠ず茆彗轭犰殓瞠妯苈┙莒犴怃徇茳苈茳弑莒犴怃徇荏蹴莒轫轸筮虢昌摒茳苈啕茔滹衄臊茳哕轭骠莒犴怃徇荏蹴莒轫轸筮虢昌摒茳苈啕氍茔滹酏茳哕轭骠苠钿犰殓瞠渝茔轸妍栳蜷蟛氨躲镱鲥荛翦ぼ屐爝鲍苠祆弑苠祆哕轭骠ず茆彗轭羼钺蝌狴妯苈│溅莒犴怃徇茳苈茳弑莒犴怃徇荏蹴莒轫轸筮虢昌摒茼狲茺苈啕爆臊茳苈啕爆臊茳弑荦苘Ζ莒犴怃徇荏蹴莒轫轸筮虢昌摒茼狲茺苈啕氍饼茳苈啕氍饼茳弑荦苠钿羼钺蝌狴渝茔轸妍忾孱舶背灬篌稞犷茔轸妍栳蜷蟛氨躲镱鲥荛翦ぼ屐爝鲍苠祆擢ず茆彗轭犰殓瞠妯苈┙莒犴怃徇茳苈茳弑莒犴怃徇茳苈茳擢苠钿犰殓瞠ピ桢箴狎箦犷祜蜥铍疱钺祠栳忮孱鏖溴禊篝蹁殄洚蝈灬糸鲥禊蝈沐铘蝈驽蝈钽轶茔轸妍祯舶脖犷蝈灬翦篝蹁殄汜忮骘躅轭翳蝈驽蝈钽弩翳弪彘町渝茔轸妍祯舶脖犷翳蝈驽蝈钽弩翳弪彘町苠钿轸屙辁妪族蝈磲螂翳狒犰镦翳弩疱钺祠殄狎骘蝽蹯狒邃轭簌眄弭蜷疳趑弪瞵楫瀹ゆㄜ漏芥ㄜ萝趄犷螬ぎ澡躞翳骈钺箫祯糸镱鏖祆忮簌眄弭蜷磲趄轼蒸殪辁轭翳彐骈汩孱骘蝽蹯狒轱麇镡翎轭邃轭序镳矬轸轱茯彐痱镳饼麇铒轭趄镤蹉犷聊屯犰顼蜷翳骘箫祧轭翳珏铄蜥疱钺扉邃聃徜蜥糸蝈珧弩箝镱痱镡戾苠耱彐瘃螨荏踱箦泗轱铥聊屯犰顼蜷翳睚昨轸轭翳篑踽蝈祜篌骢钽糸镱茆彗轭犰殓瞠孢皑苈┙苕蜥沱饼差荏蹴啕榻饼揞唛茗唛荇蜥铙苈茗唛┺铂苠钿犰殓瞠麇篝蹁翳珏铄蜷痱镡戾茆彗轭犰殓瞠茼轭孢皑苈┇孢报苈┇茔滹趔孢唯苈┈苠钿犰殓瞠麒弪ゆ唠ㄜ沅雉┈虢爆莒滹趔韦狎疱钺祠骢钽糸镱螽深趄镤蹉轭翳祜汜鲠蜷徕戾ぼ逻荛茼肄荇轫弩瘕が犷珈镡犰鲠蜷徕戾ぼ荛茼肄荇轫弩瘕が翳痱镡戾汜忮羼蹰鲠戾铘禊蝈黩轸翦狍翳骘祆秣轭荇屮糸酐珈镡犰泔铙孱篚痱镡戾睚茔轸屦塾邈糸镱份怙洳氨变轶趄殁豸邃茆彗轭犰殓铨莒徕屐珏畋茼轭荏蹴啕榻褒尬孢楱苈唛┈茼怙篚怅邈酤麸\B_i-\B=\mathbf{0},i=0,1,\ldots, N.
\end{align} 
The augmented Lagrangian of \eqref{gen1} is
\begin{align*}
L(\B_0,\ldots,\B_N, \B, \bU_0,\ldots,\bU_N)=\sum_{i=0}^N \left\{f_i(\B_i)+ \frac{\rho}{2} \|\B_i-\B+\bU_i\|^2_2   \right\}.
\end{align*}
For a given solution $\B_i^{k}, i=0,\ldots, N$ in the $k$th iteration, 
the $(k+1)$th iteration of the ADMM algorithm is given as follow:
\begin{itemize}
 \item Step 1:  	$\B^{k+1}_i=\argmin_{\B_i} \left\{f_i(\B_i)+ \frac{\rho}{2} { \|\B_i-\B^k+\bU^k_i\|^2_2 }   \right\},     i=0,\cdots,N$;  
	\item Step 2:$\B^{k+1}=\frac{1}{N+1}\sum_{i=0}^N \left\{ \B^{k+1}_i+\bU^k_i \right\}$;
	\item Step 3:$\bU_i^{k+1}=\bU_i^k+\B_i^{k+1}-\B^{k+1},i=0,\cdots,N$.
\end{itemize}
If we start with $\sum \bU^1_i=\mathbf{0}$, it can be shown that $\sum \bU^k_i=\mathbf{0}$ for every $k>1$ and so Step 2 will simply be an average operator, i.e.,
\begin{align*}
	\B^{k+1}=\frac{1}{N+1}\sum_{i=0}^N \B^{k+1}_i.
\end{align*}
 As we can see, the computational complexity of the algorithm is usually dominated by the first step. 

In general, for a convex function $f(\cdot)$, the proximal operator \citep{parikh2014proximal} is defined as: 
\begin{align} \label{l2proc}
 \proc_{f, \rho}(\A) 	\defby  \argmin_{\B}	f(\B)+\frac{\rho}{2} \|\B-\A\|_2^2  .
\end{align}
  Thus, given $\B^k$ and the $\bU_i^k$'s,  Step 1 is a proximal operator for the sum of the squared loss function $f_0(\cdot)$ and the penalty functions $f_i(\cdot),i=1,\ldots,N$. In Proposition \ref{prop1} we have derived an efficient form for the proximal operator of the squared loss $f_0(\cdot)$ at $\A={\bf 0}$. For a general $\A$ in \eqref{l2proc}, the efficient solution can be obtained by  setting $\lambda=\rho/2$ and updating $\aD$ as $n^{-1}\sum_{i=1}^n y_i \x_i \x_i \trans+\A$ in Proposition \ref{prop1}.  In next subsection, we provide the proximal operator for each penalty function. 


   


%{\color{red} How do we choose $\rho$? 1? $\rho$ in Equation (12)? }

\subsection{Proximal operator}
 

For most penalty functions, the proximal projection has an explicit solution, and we summarize these operators in this section. With some abuse of notation, let $\B$ be a parameter matrix with dimension $p\times q$. For the $\ell_1$ norm, writing $\A=(A_{ij})_{p \times q}$, we have
\begin{align*}
	\argmin_{\B}  \lambda \|\B\|_1+\frac{1}{2} \|\B-\A\|_2^2=\left( \sign(A_{ij})( |A_{ij}|-\lambda)_{+} \right)_{p \times q}\defby \soft(\A,\lambda),
\end{align*}
where $x_{+}=\max(0,x)$. For the nuclear norm, denoting the singular value decomposition of $\A$ as  
\begin{align*}
	\A=\sum_{i=1}^{\min(p,q)} \sigma_i \bu_i \bv_i\trans,	
\end{align*}
we have
\begin{align*}
	\argmin_{\B}  \lambda \|\B\|_*+\frac{1}{2} \|\B-\A\|_2^2=\sum_{i=1}^{\min(p,q)} (\sigma_i-\lambda)_{+} \bu_i \bv_i\trans.
\end{align*}

For other penalties imposed on the columns or the rows of $\B$,  we present the solutions in the form of vectors for brevity. Considering the proximal operator, 
\begin{align*}
	\widehat{\bb}=\argmin_{\bb}	f(\bb)+\frac{1}{2} \|\bb-\ba\|_2^2,\ba,\bb \in \mR^q,
\end{align*}
we have the following solution.
\begin{itemize}
\item $\ell_2$ norm--Group LASSO \citep{yuan2006model}:
\begin{align*}
	f(\bb)=\lambda \|\bb\|_2 ,\widehat{\bb}=\left(1-\frac{\lambda}{ \|\ba\|_2} \right)_{+} \cdot \ba.
\end{align*}
\item  $\ell_\infty$ norm penalty \citep{duchi2009efficient}:
\begin{align*}
	f(\bb)=\lambda \|\bb\|_\infty.
\end{align*}
When $\lambda \geq \|\ba\|_1$, we have $\hat{\bb}=\bzero$. Otherwise, the solution is 
\begin{align*}
	\widehat{\bb}=\ba-\soft(\ba,\lambda_1),
\end{align*}
where $\lambda_1 \geq 0$ satisfies the equation
\begin{align*}
	\sum_{i=1}^q (|a_i|-\lambda_1) I(|a_i|>\lambda_1)=\lambda.
\end{align*}
The details of the derivation can be found in Section 5.4 of \cite{duchi2009efficient}.
\item Hybrid $\ell_1/\ell_\infty$ norm penalty  \citep{haris2016convex}:
\begin{align*}
f(\bb)=\lambda \max \left(|b_1|, \sum_{i=2}^q |b_i|\right).
\end{align*} 
The solution is 
\begin{align*}
	\widehat{\y}=\left(\soft(a_1,\lambda_1), \soft(\ba_{-1},\lambda-\lambda_1)\right),
\end{align*}
where 
\begin{align*}
	\lambda_1=\argmin_{t \in [0, \lambda]} \|\soft(a_1,t)\|_2^2+\| \soft(\ba_{-1},\lambda-t)\|_2^2.
\end{align*}
In particular, when $\lambda \geq |a_1|+\|\ba_{-1}\|_\infty$, $\widehat{\bb}=\bzero$. Further details can be found in \cite{haris2016convex}.
\end{itemize}


With these explicit proximal operators, we can get the unified algorithm as follows.
 \begin{algorithm}[H]\small 
	\caption{HiQR: High dimensional Quadratic Regression.}
	\begin{algorithmic}[1]
		\item[Initialization:]
		\State  Input the observations $(\x_i,y_i),i=1,\cdots,n$;
		\State  Set the loss function $f_0(\cdot)$ and the penalty functions $f_1(\cdot),\cdotsf_N(\cdot)$;
		\State  Start from $k=0$, $\B^{0}_i=\aU_i^0=\bzero_{p \times p}$.
		\item[Iteration:] 	
		\State  Update $\B^{k+1}_i=\proc_{f_i,\rho}(\B-\aU_i),i=0,\cdots,N$.
		\State Update $\B^{k+1}=\frac{1}{N+1}\sum_{i=0}^N \B^{k+1}_i$.
		\State Update $\aU_i^{k+1}=\aU_i^k+\B_i^{k+1}-\B^{k+1}i=0,\cdots,N$.
		\State 	Repeat steps 4-6 until convergence. 		
		\item[Output:] Return $\B$.
	\end{algorithmic}
\end{algorithm}
Algorithm 1 is simple and efficient owing to the fact that each step of the iteration has a closed form, and we have greatly utilized the matrix structure of the problem to obtain a closed-form solution for the proximal operator of the squared loss for quadratic regression. The algorithm is fully matrix-based, where we update $p \times p$ matrices in each step without any unnecessary matrix operations such as vectorization and Kronecker product. This can greatly reduce the memory and computational burden when handling high-dimensional data.
 



\section{Simulations}
To illustrate the efficiency of the proposed algorithm, we consider a toy example: 
\begin{align*}
Y=2X_1-2X_5+2X_{10}+3X_1X_5-2.5X_5^2+4X_5X_{10}+\epsilon.	
\end{align*}
For all the simulations, we generate $\x_1,\cdots,\x_n$ independently from $N(\bzero,\bSig)$, where $\bSig=(0.5^{|k-l|})_{p \times p}$, and the error term $\epsilon$ from $N(0,1)$.  We fix the sample size $n=1000$, and vary the data dimension $p$ from small to large.  The code is implemented on an Apple M1 chip with 8-core CPUs and 8G RAM, and the R version used is 4.2.3 with vecLib BLAS.

\subsection{Ridge regression}
In this part, we compare four algorithms for computing the ridge-penalized quadratic regression, namely, the naive inverse \eqref{sol}, the Woodbury trick \eqref{wood}, the SVD method \eqref{svd}, and the proposed HiQR. We fix $\lambda=10$, and the computation times are recorded in seconds based on 10 replications.

% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com
% Date and time: Mon, Apr 17, 2023 - 10:48:20
\begin{table}[!htbp] \centering 
	\caption{Average computation time (standard deviation) of different algorithms for ridge regression ($\lambda=10$) over 10 replications. Time is recorded in seconds.  } 
	\label{tab2} 
	\resizebox{1\textwidth}{!}{%		
  \begin{tabular}{@{\extracolsep{5pt}} cccccc} 
  \\[-1.8ex]\hline 
  \hline \\[-1.8ex] 
  & p=100 & p=200 & p=400 & p=800 & p=1200 \\ 
  \hline \\[-1.8ex] 
  Naive & 9.808 (0.113) & NA & NA &  NA & NA \\ 
  Woodbury & 0.129 (0.005) & 0.557 (0.043) &  2.214 (0.095) & 21.551 (2.892) & NA \\ 
  SVD & 0.547 (0.014) & 2.685 (0.042) & 13.755 (0.167) & 72.081 (1.798) & NA \\ 
  HiQR & 0.019 (0.001) & 0.021 (0.001) &  0.023 (0.002) &  0.047 (0.005) & 0.051 (0.005) \\ 
  \hline \\[-1.8ex] 
  \multicolumn{6}{l}{*NA is produced due to out of memory in R.}
  \end{tabular} 
	}
  \end{table} 

From Table \ref{tab2}, we can observe that our HiQR algorithm greatly outperforms other algorithms in terms of computation efficiency.  Additionally, the results are roughly consist with their native computational complexity, e.g., $O(p^6)$, $O(n^2p^2+n^3)$, $O(n^2p^2)$ and $O(np^2+n^3)$. As we can see, the vectorization methods all fail to handle the $p=1200$ case due to   memory shortage, while our method is still efficient, as we only need to handle the storage of $n \times p$ and $p \times p$ matrices.     

\subsection{Single penalty function}
In this part, we investigate the performance of the proposed HiQR for a single penalty, i.e., $f(\B)=\lambda \|\B\|_1$. As a comparison, we also implement the all-pairs LASSO of vectorized features using two state-of-the-art algorithms, e.g., ``glmnet" \citep{friedman2010regularization} and ``ncvreg'' \citep{breheny2011coordinate}. Table \ref{tab3} reports the computation times of these three algorithms for a solution path with 50 $\lambda$s based on 10 replications. From Table \ref{tab3}, we can see that while the proposed HiQR is comparable to the two well-established algorithms, both ``glmnet" and ``ncvreg" fail to generate solutions when $p=1200$ due to out-of-memory errors.

\begin{table}[!htbp] \centering  
	\caption{Average computation time (standard deviation) of three packages for obtaining a solution path for all-paris LASSO over 10 replications. The same set of 50 $\lambda$s has been used for the three different packages, and time is recorded in seconds.} 
	  \label{tab3} 
	\resizebox{1\textwidth}{!}{%			  
	\begin{tabular}{@{\extracolsep{5pt}} cccccc} 
		\\[-1.8ex]\hline 
		\hline \\[-1.8ex] 
		 & p=100 & p=200 & p=400 & p=800 & p=1000 \\ 
		\hline \\[-1.8ex] 
		glmnet & 0.235(0.205) & 0.745(0.127) & 3.342(0.268) & 21.167(1.207) &  NA \\ 
		ncvreg & 0.373(0.063) & 1.421(0.129) & 6.136(0.102) & 35.162(2.004) &  NA \\ 
		HiQR & 0.324(0.087) & 0.987(0.171) & 5.475(1.308) & 26.215(7.535) & 31.319(17.579) \\ 
		\hline \\[-1.8ex] 
		\end{tabular} 
} 
\end{table} 
 
We remark that both ``glmnet" and ``ncvreg'' are  accelerated by using strong rules; see \cite{tibshirani2012strong} and \cite{lee2015strong} for more details. Strong rules screen out a large number of features to substantially improve computational efficiency. However, as \cite{tibshirani2012strong} has pointed out, the price is that ``the strong rules are not foolproof and can mistakenly discard active predictors, that is, ones that have nonzero coefficients in the solution."   As a comparison, our algorithm can be as efficient as ``glmnet" and ``ncvreg'' without the need for the same type of acceleration.  

\subsection{Hybrid penalty functions}
In this part, we report the performance of HiQR for hybrid penalty functions. Specifically, we conduct simulations for the $\ell_1+\ell_2$, $\ell_1+\ell_\infty$, $\ell_1+\ell_1/\ell_\infty$, and $\ell_1+\ell_*$ penalties. The two parameters $\lambda_1$ and $\lambda_2$ are determined by $\lambda$ and $\alpha \in(0,1)$, that is,
\begin{align*}
	\lambda_1=\lambda\cdot\alpha\cdot\lambda_{1,max},\lambda_2=\lambda\cdot(1-\alpha)\cdot\lambda_{2,max},
\end{align*}  
where $\lambda_{1,max}$ ($\lambda_{2,max}$) is set to be the smallest tuning value corresponding to a zero estimation when $\lambda_2$ $(\lambda_1)$ is set to be 0. We apply HiQR over a $10 \times 50$ grid of $(\alpha,\lambda)$ values, and Table \ref{tab4} presents the average computation times for the whole procedure. We can see that the proposed algorithm scales very well to high-dimensional quadratic regression.

 

\begin{table}[!htbp] \centering  
	\caption{Average computation times (standard deviation) of ``HiQR" under hybrid penalties with 500 tuning pairs over 10 replications.  Time is recorded in seconds.} 
	  \label{tab4} 
	\resizebox{1\textwidth}{!}{%			  
	  \begin{tabular}{@{\extracolsep{5pt}} ccccc} \\[-1.8ex]
	 \hline \hline \\[-1.8ex] 
&  $\ell_1+\ell_2$ &  $\ell_1+\ell_\infty$ &  $\ell_1+\ell_1/\ell_\infty$&  $\ell_1+\ell_*$\\
	$p=50$ &  8.947(0.735) &  5.113(0.495) &  7.654(0.519) & 14.107(1.038) \\ 
	$p=100$ & 25.963(2.952) & 10.616(1.194) & 21.891(1.964) & 46.136(2.897) \\ 
	$p=200$ & 132.924( 6.018) &  28.047( 2.152) & 105.835( 6.322) & 204.892(11.087) \\ 	
	\hline \\[-1.8ex] 
\end{tabular}} 
\end{table} 


\subsection{Model performance}
Lastly, we evaluate different penalties on different models. In particular, we consider  
\begin{align}
	\mbox{ Model 1:}&Y=2X_1-2X_5+2X_{10}+3X_1X_5-2.5X_5^2+4X_5X_{10}+\epsilon, \label{m1} \\
	\mbox{ Model 2:}&Y=-2X_5+3X_1X_5-2.5X_5^2+4X_5X_{10}+\epsilon, \label{m2}	\\
	\mbox{ Model 3:}&Y=3X_1X_5-2.5X_5^2+4X_5X_{10}+\epsilon, \label{m3}		
	\end{align}
	where the true parameters of $\B[(1,2,6,11),(1,2,6,11)]$ are 
	\begin{align*}
	\begin{pmatrix}
		0&1&-1&1\\
		1&0&1.5&0\\
		-1&1.5&-2.5&2\\
		1&0&2&0
	\end{pmatrix},\begin{pmatrix}
		0&0&-1&0\\
		0&0&1.5&0\\
		-1&1.5&-2.5&2\\
		0&0&2&0
	\end{pmatrix},\begin{pmatrix}
		0&0&0&0\\
		0&0&1.5&0\\
		0&1.5&-2.5&2\\
		0&0&2&0
	\end{pmatrix}.
	\end{align*}
	respectively. 
In particular, Model 1 has a strong hierarchical structure, Model 2 has a weak hierarchical structure, and Model 3 is a model with only interactions.


Due to the efficiency of HiQR, we studied a high-dimensional case where $p=200$ and $n=500$. It is noted that the model has about { $2\times 10^4$ parameters}. We implemented the penalized quadratic regression with 50 $\alpha$s and 50 $\lambda$s, resulting in a solution path for 2500 grids. To measure each estimation $\widehat{\B}$, we adopted the critical success index (CSI), which can evaluate the support recovery rate and the model size simultaneously. For the true $\B$ and an estimation $\widehat{\B}$, the CSI is defined as follows:
\begin{align*}
	\mbox{CSI}(\B,\widehat{\B})=\frac{\#\{(i,j): B_{ij} \neq 0\mbox{and}\hat{B}_{ij} \neq 0 \}}{\#\{(i,j): B_{ij} \neq 0\mbox{or}\hat{B}_{ij} \neq 0 \}}.
\end{align*}
Figure \ref{fig1} presents the results for different models and different penalties.  From these solution paths, we can see that these methods can detect the true signals if the tuning parameters are set suitably. 
{Although tuning parameters selection is beyond the scope of the current work, our results indicate that the proposed "HiQR" algorithm is capable of training a model with $2 \times 10^4$ parameters and 2500 tuning parameters efficiently, and is able to generate estimations with satisfactory support recovery (c.f. grids in black in Figure \ref{fig1}). 
} 
  \begin{figure}[!ht]
	\centerline{
		\begin{tabular}{cccc}				
			\multicolumn{4}{c}{Strong hierarchical model \eqref{m1}}\\
			\psfig{figure=500-200-1-12.pdf,width=1.25 in,height=1.25 in,angle=0} &
			\psfig{figure=500-200-1-13.pdf,width=1.25 in,height=1.25 in,angle=0} &
			\psfig{figure=500-200-1-14.pdf,width=1.25 in,height=1.25 in,angle=0} &
			\psfig{figure=500-200-1-15.pdf,width=1.25 in,height=1.25 in,angle=0} \\
			$\ell_1+\ell_2$ &  $\ell_1+\ell_\infty$ &  $\ell_1+\ell_1/\ell_\infty$&  $\ell_1+\ell_*$\\		
			\hline
			\multicolumn{4}{c}{Weak hierarchical model \eqref{m2}}\\
			\psfig{figure=500-200-2-12.pdf,width=1.25 in,height=1.25 in,angle=0} &
			\psfig{figure=500-200-2-13.pdf,width=1.25 in,height=1.25 in,angle=0} &
			\psfig{figure=500-200-2-14.pdf,width=1.25 in,height=1.25 in,angle=0} &
			\psfig{figure=500-200-2-15.pdf,width=1.25 in,height=1.25 in,angle=0} \\
			$\ell_1+\ell_2$ &  $\ell_1+\ell_\infty$ &  $\ell_1+\ell_1/\ell_\infty$&  $\ell_1+\ell_*$\\		
			\hline
			\multicolumn{4}{c}{Pure interaction model \eqref{m3}}\\	
			\psfig{figure=500-200-3-12.pdf,width=1.25 in,height=1.25 in,angle=0} &
			\psfig{figure=500-200-3-13.pdf,width=1.25 in,height=1.25 in,angle=0} &
			\psfig{figure=500-200-3-14.pdf,width=1.25 in,height=1.25 in,angle=0} &
			\psfig{figure=500-200-3-15.pdf,width=1.25 in,height=1.25 in,angle=0} \\
			$\ell_1+\ell_2$ &  $\ell_1+\ell_\infty$ &  $\ell_1+\ell_1/\ell_\infty$&  $\ell_1+\ell_*$\\		
			\hline
		\end{tabular}
	}
	\caption{The critical success index for different models and different penalties with 2500 tuning parameters.}
	\label{fig1}
\end{figure}



%%We are grateful to the Editor, the Associate Editor and the two referees for their constructive comments, which helped us to improve the manuscript.  \section*{Acknowledgments}
%Wang's research is partially supported by NSFC 12031005, NSF of Shanghai 21ZR1432900 and the fundamental research funds for the central universities.  Jiang's research is partially supported by the National Natural Science Foundation of China (12001459), and HKPolyU Internal Grants.

\bibliography{ref}
\end{document}

